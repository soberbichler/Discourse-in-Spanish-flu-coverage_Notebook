{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover media discourse in Spanish flu coverage with diachronic bi-gram clouds and topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spanish flu of 1918/19 is known as a influenza pandemic that spread in three waves over Europe, Asia and North America and then over the southern hemisphere in a period of around 13 months, probably infecting a third of the world's population (around 500 million people) and causing around 50 million deaths. Since all influenza A epidemics since then have been identified as descendants of the 1918 virus, the Spanish flu is often called the \"mother of all pandemics.\" \n",
    "\n",
    "<img src=\"images/tm.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Media coverage was hampered by the events of the war. Newspaper reports continued to be censored, especially with regard to the first wave in late spring and early summer 1918, which was only recognized at a late stage, but also the second, extremely fatal wave between September and November of the same year. The third, far less violent wave was then little reported, despite the end of the war. In Europe, the epidemic was always analyzed in the context of the war, meaning that it was interpreted as a \"footnote of World War I\" (L. Spinney, Pale Rider, 2017). In fact, more people died here as a result of the war, or death was still seen by many as a consequence of the war.\n",
    "\n",
    "In this notebook, we examine news coverage of the Spanish flu in French, German, or Finnish corpora. The notebook computes diachronic ngrams and topic models that allow us to take a first look at the collection and find possible *discourse markers* for further, qualitative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step...\n",
    "\n",
    "* [Run code](#1-bullet)\n",
    "* [Import your newspaper collection](#2-bullet)\n",
    "* [Create a frequency graph using the publication date and the newspaper title](#3-bullet)\n",
    "* [Clean, tokenize and stem (pre-processing)](#4-bullet)\n",
    "* [Add part of speach (POS) tags, form ngrams and lemmatize](#5-bullet)\n",
    "* [How did the discourse on the Spanish Flu change over the months?](#6-bullet)\n",
    "* [What tpics can you find the in the newspaper coverage on the Spanish Flu?](7#-bullet)\n",
    "* [Change or adapt parameters](#8-bullet)\n",
    "* [Browse original texts](#9-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code <a class=\"anchor\" id=\"1-bullet\"></a>\n",
    "\n",
    "In order to run code in Jupyter Notebooks, you need to click on \"Run\" in the tool bar, which is above the main notebooks area or use the command mode shortcut \"shift-enter\". When you \"run\" a cell, an asterisk appears in the parentheses next to the cell. This cell was successfully executed when a digit appears in the parentheses. \n",
    "<img src=\"images/run.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#languange processing imports\n",
    "import gensim, spacy, logging, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import time\n",
    "from gensim.models import LdaModel\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#For the text overview\n",
    "from nltk import FreqDist\n",
    "\n",
    "#Word Cloud and Visualization\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your newspaper collection <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    "\n",
    "Now you can add your exported dataset to the file in the entry page. You can directly import the saved dataset from your downloads. Then click \"upload\" and copy the full file name of the dataset. You can either import the French, the German or the Finnish dataset. \n",
    "\n",
    "![run](images/binder.png)\n",
    "\n",
    "Paste the file name into the empty brackets of \"df = pd.read_csv()\" and put quotes before and after the file name: ('export_espanjantauti_09_04_2021_14_48.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_german = pd.read_csv()\n",
    "df_text = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a frequency graph using the publication date and the newspaper title  <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "    \n",
    "Here you can find out which newspapers reported about the topic Spanish flu and how often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,50))\n",
    "df['date']=pd.to_datetime(df['date']).dt.strftime('%Y-%m')\n",
    "\n",
    "\n",
    "fig = df.groupby(['date','newspaper_id']).size().unstack().plot(kind='bar',stacked=True)\n",
    "plt.savefig('bar.png', dpi = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, tokenize and stem (pre-processing) <a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "\n",
    "Before you can start training the bi-gram and tri-gram models, you will need to pre-process your texts. The following functions remove punctuation, lowercase the text, remove stop words, and stem the text.\n",
    "\n",
    "To successfully remove stop words, it is important to specify the language of your dataset. If you imported the German dataset, add **\"german\"** as the language, if you are working with the French dataset, add **\"french\"**, and finally, if you are working with the Finnish dataset, add **\"finnish\"**. You can also expand the list of stop words. For many research questions, it can be helpful to remove the search terms and words that are very common but have no relevance to your topic. Copy and paste the listed additional stop words into the bracklets. \n",
    "\n",
    "\n",
    "<img src=\"images/stopword.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean, tokenize, and stem the data\n",
    "def initial_clean(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower() \n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('')#change the language here\n",
    "# list for additional German words that should be considered as stop words: 'ganz', 'ganzen', 'ganze', 'Grippe', 'krankheit', 'Krankheit', 'Krankheiten','Influenza', 'spanischen', 'spanische'\n",
    "# list for additional French words that should be considered as stop words: 'grippe', 'espagnole'\n",
    "# list for additional Finnish words that should be considered as stop words: 'espanjantauti'\n",
    "stop_words.extend([])\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] \n",
    "    except IndexError: \n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_stop_words(initial_clean(text)))\n",
    "\n",
    "df['tokenized'] = df['text'].apply(apply_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add part of speach (POS) tags, form ngrams and lemmatize <a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "\n",
    "A POS tag (or part-of-speech tag) is a special label assigned to each token (word) in a dataset to indicate the part of speech. In this notebook we are using the tags *noun*, *adjective*, *adverb* and *verb*. You can use the tags to narrow down your dataset to specific part-of-speeches, for example, it is possible to see the most used nouns or verbs, depending on what makes sense for the type of dataset. \n",
    "The code in the cell below will also form ngrams, and lemmatize the tokenized words. Lemmatization reduces inflected words in order to be able to group them.\n",
    "\n",
    "#### Language for your dataset\n",
    "\n",
    "For the POS tagging, you need to chose the language of your the dataset. If you are using the German dataset, keep **'de_core_news_sm'**, if you are using the Frech dataset, use **'fr_core_news_sm'**. Spacy does not have a model for the Finnish language yet. If you are using the Finnish dataset, you simply need to disable POS tagging and lematization. You can do this by adding **'tagger'** and **'lemmatize'** to the functions that will be disabled. You also need to remove following part of the code: **if token.pos_ in allowed_postags**. \n",
    "\n",
    "<img src=\"images/tagging.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'ADV', 'VERB']):\n",
    "    \"\"\"Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts_out = []\n",
    "    \n",
    "    nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready1 = process_words(df['tokenized'])  # processed Text Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did the discourse on the Spanish flu change over the months? <a class=\"anchor\" id=\"6-bullet\"></a>\n",
    "\n",
    "Here you can create diachronic Ngram clouds for specific years or time frames.\n",
    "\n",
    "#### Run the cell while you read the explanations on Ngrams. It will take on or two minutes to load. \n",
    "\n",
    "Ngrams are connected sequences of n items from a given text or speech sample. This means that words are not considered as individual units, but in relation to each other. For scholars in the humanities, ngrams can be helpful to get an overview of their collection or to identify discourse markers (discourse = a group of related texts belonging to a common system of formation). Ngrams can never be a research result per se - which is true for any output of NLP methods - but they can help to find important patterns in a particular collection.\n",
    "\n",
    "If ngrams are used to identify discourse markers, it can be useful to create diachrinic ngrams to explore the change of rextual patterns. This Notebook therefore shows how diachronic ngrams can be build and visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['month']=(pd.to_datetime(df['date']).dt.strftime('%m')) # year = %Y, month = %m\n",
    "df['month'] = df['month'].astype(int)\n",
    "\n",
    "july_1918 = (df['month'] > int('6')) & (df['month'] <= int('7'))\n",
    "august_1918 = (df['month'] > int('7')) & (df['month'] <= int('8'))\n",
    "september_1918 = (df['month'] > int('8')) & (df['month'] <= int('9'))\n",
    "october_1918 = (df['month'] > int('9')) & (df['month'] <= int('10'))\n",
    "november_1918 = (df['month'] > int('10')) & (df['month'] <= int('11'))\n",
    "december_1918 = (df['month'] > int('11')) & (df['month'] <= int('12'))\n",
    "\n",
    "#This will be used for the title of the clouds\n",
    "time_frame = ['July 1918', 'August 1918', 'September 1918', 'October 1918', 'November 1918', 'December 1918']\n",
    "\n",
    "#Here you create a list containing the information on your selected time-frames\n",
    "selection = []\n",
    "selection.append(july_1918)\n",
    "selection.append(august_1918)\n",
    "selection.append(september_1918)\n",
    "selection.append(october_1918)\n",
    "selection.append(november_1918)\n",
    "selection.append(december_1918)\n",
    "\n",
    "#Get your bigrams for every selected time frame\n",
    "selection[0] = df.loc[july_1918]['tokenized']\n",
    "selection[1] = df.loc[august_1918]['tokenized']\n",
    "selection[2] = df.loc[september_1918]['tokenized']\n",
    "selection[3] = df.loc[october_1918]['tokenized']\n",
    "selection[4] = df.loc[november_1918]['tokenized']\n",
    "selection[5] = df.loc[december_1918]['tokenized']\n",
    "\n",
    "#Create the diachronic Ngram clouds\n",
    "i = -1\n",
    "while i < len(selection) -1:\n",
    "    i = i +1 \n",
    "    data_ready3 = process_words(selection[i])  # processed Text Data!\n",
    "    bigrams_list = list(nltk.bigrams(data_ready3))\n",
    "    dictionary = [' '.join([str(tup) for tup in bigrams_list])]\n",
    "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    bag_of_words = vectorizer.fit_transform(dictionary)\n",
    "    vectorizer.vocabulary_\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    words_dict = dict(words_freq)\n",
    "    WC_height = 1000\n",
    "    WC_width = 1500\n",
    "    WC_max_words = 500\n",
    "    wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width)\n",
    "    wordCloud.generate_from_frequencies(words_dict)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot\n",
    "    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "    plt.title(time_frame[i], size= 'x-large')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('cloud.png', dpi = 300)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kind of topics can you find the in the newspaper coverage on the Spanish flu? <a class=\"anchor\" id=\"7-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(data):\n",
    "    num_topics = 10\n",
    "    \n",
    "    chunksize = 200\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   chunksize=chunksize, minimum_probability=0.0, passes=20, iterations=200, per_word_topics=True, update_every=1)\n",
    "    return dictionary,corpus,lda\n",
    "\n",
    "dictionary,corpus,lda = train_lda(data_ready1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now visualize the results of the topic modeling\n",
    "\n",
    "Check out the results. Do they make sense in our eyes. Can you interpert some of the topics? Use the cell \"Brose original texts\" if you want to check out some of the words in their original context. Add your search term where you see 'flu' at the moment. You must search for the word in its original form and keep capitalization in mind. \n",
    "\n",
    "If you are using the German or French dataset, go back to step \"Add or remove part of speach (POS) tags, form Ngrams and lemmatize\" and remove \"Verb\" from the list of pos tags. Re-run the notebook and see how this changes your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "topics = lda.show_topics(formatted=False, num_words=20)\n",
    "data_flat = [w for w_list in data_ready1 for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(5, 2, figsize=(20,20), sharey=True, dpi=300)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], width=0.6, alpha=0.4, label='word count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], width=0.3, label='importance')\n",
    "    ax.set_ylabel('word count')\n",
    "    ax_twin.set_ylim(0, 0.06); ax.set_ylim(0, 150)\n",
    "    ax.set_title('Topic: ' + str(i), fontsize=20)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=40, fontsize=15, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left', fontsize=15); ax_twin.legend(loc='upper right', fontsize=15)\n",
    "\n",
    "fig.tight_layout(w_pad=12)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.08)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change or adapt parameters: <a class=\"anchor\" id=\"8-bullet\"></a>\n",
    "\n",
    "1. Remove verbs from your dataset and see how this changes your results (if you are using the German or the French dataset) \n",
    "2. Disable lemmatization while pre-processing your dataset (if you are using the German or the French dataset) \n",
    "3. Train unigrams instad of bi-grams by changing the numbers to 1 in the following line: vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "4. Change the chunksize (number of documents to be used in each training) from 200 to 50 while training your topic modeling algorithm and see how this changes TM results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse original texts <a class=\"anchor\" id=\"-9-bullet\"></a>\n",
    "\n",
    "Here you can browse through the original text. To refine the search, you can reduce the search to those texts that contain a specific word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "lines_ = []\n",
    "for line in df_text:\n",
    "    if 'flu' in line: #insert here the word you are looking for\n",
    "        lines_.append(line)\n",
    "    \n",
    "lines_ = pd.DataFrame(lines_, columns =['text'])\n",
    "df_select = pd.DataFrame(lines_['text'])\n",
    "df_select[0:10]    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
